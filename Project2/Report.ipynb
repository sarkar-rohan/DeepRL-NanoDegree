{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 : Single Agent Continuous Control for the Reacher Task\n",
    "\n",
    "---\n",
    "\n",
    "Author : Rohan Sarkar\n",
    "\n",
    "## 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.18 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the necessary libraries and software modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environments corresponding to both versions of the environment are already saved in the Workspace and can be accessed at the file paths provided below.  \n",
    "\n",
    "Please select one of the two options below for loading the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "env = UnityEnvironment(file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "# env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [  0.00000000e+00  -4.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00  -1.00000000e+01   0.00000000e+00\n",
      "   1.00000000e+00  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   5.75471878e+00  -1.00000000e+00\n",
      "   5.55726671e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "  -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Take Random Actions in the Environment \n",
    "[For testing if environment has been set up properly]\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.13999999687075615\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Solve the Reacher Task using DDPG Algorithm\n",
    "\n",
    "The DDPG Algorithm is an actor-critic, model-free, off-policy algorithm based on the deterministic policy gradient that can operate over continuous action spaces proposed in the paper Lillicrap et al, 2015. The main highlights of the algorithm are as follows: \n",
    "- It uses two neural networks -- an actor that learns a policy function from the state and a critic that learns a value function from state-action pairs. This is implemented in Section 4.2.\n",
    "- The algorithm uses experience replay to store experiences of the agent interacting with the environment and subsequently sampling mini-batches of uncorrelated experiences for training. This is implemented in Section 4.3\n",
    "\n",
    "The agent is implemented in Section 4.4 and the code for training the agent is in Section 4.5. Some important implementational details are as follows: \n",
    "- To boost exploration, Ornstein-Uhlenbeck Process is used to add noise to the action output. The goal is to generate temporally correlated exploration for exploration efficiency in physical control problems with inertia. \n",
    "- To stablize the training, soft update is used to update model weights of the **target** actor-critic network from the **local** actor-critic network. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Set the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hyper-parameters for Replay memory and generation of mini-batches of experiences\n",
    "\"\"\"\n",
    "BUFFER_SIZE = int(1e6) # replay buffer size\n",
    "BATCH_SIZE = 128       # minibatch size\n",
    "\"\"\"\n",
    "Hyper-parameters for DDPG Training\n",
    "\"\"\"\n",
    "N_EPISODES = 2000      # maximum number of training episodes\n",
    "N_MAX_TIMESTEPS = 1000 # maximum number of timesteps per episode\n",
    "N_STATE = state_size   # state size for DDPG Agent\n",
    "N_ACTION = action_size # action size for DDPG Agent\n",
    "SEED = 1               # random seed\n",
    "GAMMA = 0.99           # discount factor\n",
    "TAU = 1e-3             # for soft update of target parameters\n",
    "WEIGHT_DECAY = 0       # L2 weight decay\n",
    "LEARN_EVERY = 20       # learning timestep interval\n",
    "LEARN_NUM = 10         # number of learning passes\n",
    "GRAD_CLIPPING = 1.0    # gradient clipping \n",
    "# Ornstein-Uhlenbeck noise parameters\n",
    "OU_MEAN = 0.0\n",
    "OU_SIGMA = 0.2\n",
    "OU_THETA = 0.15\n",
    "EPSILON = 1.0         \n",
    "EPSILON_DECAY = 1e-6\n",
    "\"\"\"\n",
    "Hyper-parameters for Actor and Critic\n",
    "\"\"\"\n",
    "LR_ACTOR = 1e-3        # learning rate of the actor\n",
    "LR_CRITIC = 1e-3       # learning rate of the critic\n",
    "ACTOR_NN = [400, 200]  # hidden layer sizes of the actor network\n",
    "CRITIC_NN = [400, 200] # hidden layer sizes of the critic network\n",
    "WEIGHT_INIT = 3e-3     # initialize weights \n",
    "# Select GPU for training if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Define the Actor and Critic Network Architecture\n",
    "- The **Actor Network** learns the policy function and maps states to actions. It consists of three fully connected layers with batch normalization at the first layer. It uses ReLU activation function for the first two layers and uses tanh in the last layer to ensure that the continuous action values range between [-1, 1].\n",
    "- The **Critic Network** learns the value function and maps (state,action) pairs to Q-values. It also consists of three fully connected layers with batch normalization applied at the first layer. It uses ReLU activation function in all the layers except the last one. The last layer has no activation function so that we get the actual Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=========================================================================\n",
    "                    Actor Model - Learns Policy Function\n",
    "=========================================================================\n",
    "\n",
    "Parameters:\n",
    "-----------\n",
    "state_size (int): Dimension of each state\n",
    "action_size (int): Dimension of each action\n",
    "seed (int) : Random seed\n",
    "NN (list) : Number of nodes in the hidden layers        \n",
    "\"\"\"\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed, NN=ACTOR_NN):\n",
    "        \"\"\"\n",
    "        Initialize parameters and build model.\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, NN[0])\n",
    "        self.bn1 = nn.BatchNorm1d(NN[0])\n",
    "        self.fc2 = nn.Linear(NN[0], NN[1])\n",
    "        self.fc3 =  nn.Linear(NN[1], action_size)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def initialize(self, layer):\n",
    "        fan_in = layer.weight.data.size()[0]\n",
    "        lim = 1. / np.sqrt(fan_in)\n",
    "        return(-lim, lim)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*self.initialize(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*self.initialize(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-1*WEIGHT_INIT, WEIGHT_INIT)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Build an actor(policy) network that maps states to actions\n",
    "        tanh activation ensures that the action values range from [-1, 1]\n",
    "        \"\"\"\n",
    "        x = F.relu(self.bn1(self.fc1(state)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "\"\"\"\n",
    "=========================================================================\n",
    "                Critic Model - Learns Value Function\n",
    "=========================================================================\n",
    "Parameters:\n",
    "-----------\n",
    "state_size (int): Dimension of each state\n",
    "action_size (int): Dimension of each action\n",
    "seed (int) : Random seed\n",
    "NN (list) : Number of nodes in the hidden layers\n",
    "\"\"\"  \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed, NN=CRITIC_NN):\n",
    "        \"\"\"\n",
    "        Initialize parameters and build model.\n",
    "        \n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, NN[0])\n",
    "        self.bn1 = nn.BatchNorm1d(NN[0])\n",
    "        self.fc2 = nn.Linear(NN[0]+action_size, NN[1])\n",
    "        self.fc3 = nn.Linear(NN[1], 1)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def initialize(self, layer):\n",
    "        fan_in = layer.weight.data.size()[0]\n",
    "        lim = 1. / np.sqrt(fan_in)\n",
    "        return(-lim, lim)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*self.initialize(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*self.initialize(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        \"\"\"\n",
    "        Build a critic (value) network that maps (state,action) pairs to Q-values\n",
    "        \"\"\"\n",
    "        x = F.relu(self.bn1(self.fc1(state)))\n",
    "        x = torch.cat((x, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Define the Replay Memory \n",
    "- For storing experience tuples\n",
    "- For generating mini-batches of experiences for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=========================================================================\n",
    "            Replay Memory to store and sample experiences\n",
    "=========================================================================\n",
    "Parameters:\n",
    "-----------\n",
    "buffer_size (int): maximum size of buffer\n",
    "batch_size (int): size of each training batch\n",
    "\"\"\"  \n",
    "class ReplayBuffer:\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size) # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store new experience to replay memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory for training\"\"\"\n",
    "        experiences =  random.sample(self.memory, k=self.batch_size)\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\"Return the current size of internal memory\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Define and Instantiate the DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "=========================================================================\n",
    "            Ornsten-Uhlenbeck process to add exploration noise\n",
    "=========================================================================\n",
    "\"\"\"                  \n",
    "class ExplorationNoise:\n",
    "    def __init__(self, size, seed, mu=OU_MEAN, theta=OU_THETA, sigma=OU_SIGMA):\n",
    "        \"\"\"Initialize noise parameters\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state to mean.\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\"\"\"\n",
    "=========================================================================\n",
    "    DDPG Agent that interacts with and learns from the environment\n",
    "=========================================================================\n",
    "Parameters:\n",
    "-----------\n",
    "state_size (int): dimension of each state\n",
    "action_size (int): dimension of each action\n",
    "random_seed (int): random seed\n",
    "\"\"\"  \n",
    "class Agent():   \n",
    "    def __init__(self, state_size, action_size, random_seed=0):\n",
    "        \"\"\"Initialize an Agent object.\"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "        self.epsilon = EPSILON\n",
    "        \n",
    "        # Initialize the local and target Actor network\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "        \n",
    "        # Initialize the local and target Critic network\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "        \n",
    "        # Initialize the Ornsten-Uhlenbeck process to add exploration noise\n",
    "        self.noise = ExplorationNoise(action_size, random_seed)\n",
    "        \n",
    "        # Initialize the Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done, timestep):\n",
    "        \"\"\" Save experience in replay memory, and use random sample from buffer to learn \"\"\"\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        \"\"\" Learn, when enough samples are available in memory \"\"\"\n",
    "        if len(self.memory) > BATCH_SIZE and timestep % LEARN_EVERY == 0:\n",
    "            for _ in range(LEARN_NUM):\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "                \n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\" Returns actions for given state as per current policy \"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        \n",
    "        if add_noise:\n",
    "            action += self.epsilon * self.noise.sample()\n",
    "            \n",
    "        return np.clip(action, -1, 1)\n",
    "    \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target_model = tau*θ_local_model + (1 - tau)*θ_target_model\n",
    "        \n",
    "        where, \n",
    "            θ_local_model: PyTorch model from which weights will be copied\n",
    "            θ_target_model: PyToch model to which weights will be copied\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "        \n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where,\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        \"\"\" \n",
    "        -----------------------------------------------------------------------------------\n",
    "                                    Unpack mini-batch of experiences\n",
    "                    experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done)\n",
    "        -----------------------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        \"\"\" \n",
    "        -----------------------------------------------------------------------------------\n",
    "                                Train Critic by minimizing the loss\n",
    "        -----------------------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        # get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # compute Q targets for current states\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # minimize the critic loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        # gradient clipping for critic\n",
    "        if GRAD_CLIPPING > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), GRAD_CLIPPING)\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        \"\"\" \n",
    "        -----------------------------------------------------------------------------------\n",
    "                            Train Actor using the sampled policy gradient\n",
    "        -----------------------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        # compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        \"\"\" \n",
    "        -----------------------------------------------------------------------------------\n",
    "                                Update Actor-Critic Target Networks\n",
    "        -----------------------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)\n",
    "        # update epsilon decay\n",
    "        if EPSILON_DECAY > 0:\n",
    "            self.epsilon -= EPSILON_DECAY\n",
    "            self.noise.reset()\n",
    "\"\"\"\n",
    "-----------------------------------------------------------------------------------\n",
    "                        Instantiate the DDPG Agent\n",
    "-----------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "agent = Agent(state_size=N_STATE, action_size=N_ACTION, random_seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Train the DDPG algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50 (8s)\tMean: 3.3\tMoving Avg: 1.5\n",
      "Episode 100 (9s)\tMean: 5.1\tMoving Avg: 2.4\n",
      "Episode 150 (8s)\tMean: 5.7\tMoving Avg: 4.0\n",
      "Episode 200 (9s)\tMean: 9.6\tMoving Avg: 5.4\n",
      "Episode 250 (9s)\tMean: 9.2\tMoving Avg: 7.3\n",
      "Episode 300 (9s)\tMean: 6.7\tMoving Avg: 8.7\n",
      "Episode 350 (9s)\tMean: 8.4\tMoving Avg: 9.7\n",
      "Episode 400 (10s)\tMean: 10.6\tMoving Avg: 10.7\n",
      "Episode 450 (10s)\tMean: 18.6\tMoving Avg: 11.4\n",
      "Episode 500 (10s)\tMean: 11.8\tMoving Avg: 12.6\n",
      "Episode 550 (10s)\tMean: 13.2\tMoving Avg: 14.0\n",
      "Episode 600 (11s)\tMean: 15.7\tMoving Avg: 15.1\n",
      "Episode 650 (11s)\tMean: 15.4\tMoving Avg: 15.3\n",
      "Episode 700 (11s)\tMean: 16.9\tMoving Avg: 16.1\n",
      "Episode 750 (12s)\tMean: 15.5\tMoving Avg: 18.1\n",
      "Episode 800 (12s)\tMean: 26.3\tMoving Avg: 18.8\n",
      "Episode 850 (12s)\tMean: 26.2\tMoving Avg: 19.5\n",
      "Episode 900 (13s)\tMean: 28.7\tMoving Avg: 21.0\n",
      "Episode 950 (13s)\tMean: 23.5\tMoving Avg: 22.8\n",
      "Episode 1000 (13s)\tMean: 29.8\tMoving Avg: 24.4\n",
      "Episode 1050 (13s)\tMean: 29.4\tMoving Avg: 26.2\n",
      "Episode 1100 (13s)\tMean: 39.1\tMoving Avg: 28.7\n",
      "\n",
      "Environment solved in 1121 episodes.\tAverage score: 30.01\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=========================================================================\n",
    "                        Train the DDPG Algorithm\n",
    "=========================================================================\n",
    "Parameters:\n",
    "-----------\n",
    "num_episodes (int): maximum number of episodes for training\n",
    "max_t (int)       : maximum number of time-steps per episode\n",
    "actor_weights_name, critic_weights_name : actor and critic model names\n",
    "\"\"\"  \n",
    "def train(actor_weights_name, critic_weights_name, num_episodes=N_EPISODES, max_t=N_MAX_TIMESTEPS):\n",
    "    \"\"\" \n",
    "    -----------------------------------------------------------------------------------\n",
    "    Initialize data-structures to keep track of average-episode scores and moving average\n",
    "    -----------------------------------------------------------------------------------\n",
    "    \"\"\" \n",
    "    mean_scores = [] # list of mean scores from each episode\n",
    "    moving_avgs = [] # list of moving averages\n",
    "    best_score = -np.inf\n",
    "    scores_window = deque(maxlen=100) # mean score from last 100 episodes based on Udacity Project requirements\n",
    "    \"\"\" \n",
    "    -----------------------------------------------------------------------------------\n",
    "    Start learning\n",
    "    -----------------------------------------------------------------------------------\n",
    "    \"\"\" \n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        \"\"\" Reset environment and agent at the beginning of each episode \"\"\"\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset environment, with training mode ON\n",
    "        scores = np.zeros(num_agents) # initialize score for each agent\n",
    "        states = env_info.vector_observations # get current state \n",
    "        agent.reset() # reset agent\n",
    "        start_time = time.time()\n",
    "        \"\"\" Start episode \"\"\"\n",
    "        for t in range(max_t):\n",
    "            \"\"\" Agent interacts with environment \"\"\"\n",
    "            actions = agent.act(states, add_noise=True) # determine optimal actions\n",
    "            env_info = env.step(actions)[brain_name]  # apply actions to environment and get environment info\n",
    "            next_states = env_info.vector_observations # extract next state information for each agent\n",
    "            rewards = env_info.rewards # extract rewards received for the action taken by each agent\n",
    "            dones = env_info.local_done # Determine terminal state or not\n",
    "            \"\"\" Agent learns from uncorrelated experiences \"\"\"\n",
    "            for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "                agent.step(state, action, reward, next_state, done, t)\n",
    "            states = next_states\n",
    "            scores += rewards\n",
    "            if np.any(dones):\n",
    "                break\n",
    "        duration = time.time() - start_time\n",
    "        \"\"\" \n",
    "        -----------------------------------------------------------------------------------\n",
    "        Record score stats and print information\n",
    "        -----------------------------------------------------------------------------------\n",
    "        \"\"\" \n",
    "        mean_scores.append(np.mean(scores)) # save mean score for each episode\n",
    "        scores_window.append(mean_scores[-1]) # save mean score to window\n",
    "        moving_avgs.append(np.mean(scores_window)) # save moving average\n",
    "        \n",
    "        if i_episode % 50 == 0:\n",
    "            print(\"\\rEpisode {} ({}s)\\tMean: {:.1f}\\tMoving Avg: {:.1f}\"\\\n",
    "                  .format(i_episode, round(duration), mean_scores[-1], moving_avgs[-1]))\n",
    "        \"\"\" \n",
    "        -----------------------------------------------------------------------------------\n",
    "        Save model weights when the task is solved (moving average score >= 30.0)\n",
    "        -----------------------------------------------------------------------------------\n",
    "        \"\"\" \n",
    "        if moving_avgs[-1] >= 30.0:\n",
    "            print(\"\\nEnvironment solved in {:d} episodes.\\tAverage score: {:.2f}\"\\\n",
    "                 .format(i_episode, moving_avgs[-1]))\n",
    "            torch.save(agent.actor_local.state_dict(), actor_weights_name)\n",
    "            torch.save(agent.critic_local.state_dict(), critic_weights_name)\n",
    "            break        \n",
    "    return mean_scores, moving_avgs\n",
    "\"\"\" \n",
    "-----------------------------------------------------------------------------------\n",
    "Train and save final model weights\n",
    "-----------------------------------------------------------------------------------\n",
    "\"\"\" \n",
    "scores, avgs = train('DDPG_actor_single.pth', 'DDPG_critic_single.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results and Plot of Rewards\n",
    "As indicated above the enviroment was solved in 1121 episodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f70b82ae3c8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(np.arange(len(scores)), scores, label='Actual Score ')\n",
    "plt.plot(np.arange(len(scores)), avgs, c='r', label='Moving Average')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode Number')\n",
    "plt.title('Reacher Task using DDPG - Score vs Episode')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ideas for Future Work\n",
    "- Would like to look into ways to speed up training. \n",
    "- Parallelize implementation with multiple agents interacting with the environment independently. More specifically, I would like to explore distributed policy gradient algorithms such as A3C, D4PG. \n",
    "- Replace the Replay Memory with Proritized Experience Replay for better performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
