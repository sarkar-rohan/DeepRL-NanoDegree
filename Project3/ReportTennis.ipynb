{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 : Multi-Agent Collaboration for the Tennis Task\n",
    "\n",
    "---\n",
    "Author : Rohan Sarkar \n",
    "\n",
    "## 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the necessary libraries and software modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is already saved in the Workspace and can be accessed at the file path provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"/data/Tennis_Linux_NoVis/Tennis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for both agents look like: [[ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.         -6.65278625 -1.5        -0.          0.\n",
      "   6.83172083  6.         -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.         -6.4669857  -1.5         0.          0.\n",
      "  -6.83172083  6.          0.          0.        ]]\n",
      "The rewards for both agents look like: [0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for both agents look like:', states)\n",
    "\n",
    "rewards = env_info.rewards\n",
    "print('The rewards for both agents look like:', rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):                                         # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Solve the Tennis Task using DDPG Algorithm for multiple agents\n",
    "\n",
    "The DDPG Algorithm is an actor-critic, model-free, off-policy algorithm based on the deterministic policy gradient that can operate over continuous action spaces proposed in the paper Lillicrap et al, 2015. \n",
    "\n",
    "The main highlights of the algorithm are as follows:\n",
    "- It uses two neural networks -- an actor that learns a policy function from the state and a critic that learns a value function from state-action pairs. This is implemented in Section 4.2.\n",
    "- The algorithm uses experience replay to store experiences of the agent interacting with the environment and subsequently sampling mini-batches of uncorrelated experiences for training. This is implemented in Section 4.3\n",
    "\n",
    "The agent is implemented in Section 4.4 and the code for training the agent is in Section 4.5. Some important implementational details are as follows:\n",
    "- To boost exploration, Ornstein-Uhlenbeck Process is used to add noise to the action output. The goal is to generate temporally correlated exploration for exploration efficiency in physical control problems with inertia.\n",
    "- To stablize the training, soft update is used to update model weights of the target actor-critic network from the local actor-critic network.\n",
    "\n",
    "To adapt the DDPG algorithm for this collaborative task please note the following: \n",
    "- each of the two agents uses the same actor network and receives its own local observation\n",
    "- the experience of each agent is added to a shared replay buffer from which mini-batches of experiences are sampled for training\n",
    "- the reward for both the agents is the maximum of the rewards collected by each of the agents, which signifies the maximum time the ball was in the air. \n",
    "\n",
    "It is for these reasons that the code can easily be adapted to simultaneously train both agents through self-play. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Set the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORE_SOLVED = 0.5    # Moving average score when the environment is considered solved\n",
    "\"\"\"\n",
    "Hyper-parameters for Replay memory and generation of mini-batches of experiences\n",
    "\"\"\"\n",
    "BUFFER_SIZE = int(1e6) # replay buffer size\n",
    "BATCH_SIZE = 256       # minibatch size\n",
    "\"\"\"\n",
    "Hyper-parameters for DDPG Training\n",
    "\"\"\"\n",
    "N_EPISODES = 2000      # maximum number of training episodes\n",
    "N_MAX_TIMESTEPS = 1000 # maximum number of timesteps per episode\n",
    "N_STATE = state_size   # state size for DDPG Agent\n",
    "N_ACTION = action_size # action size for DDPG Agent\n",
    "SEED = 1               # random seed\n",
    "GAMMA = 0.99           # discount factor\n",
    "TAU = 1e-3             # for soft update of target parameters\n",
    "LEARN_INTERVAL = 10    # learning timestep interval\n",
    "LEARN_NUM = 10         # number of learning passes\n",
    "GRAD_CLIPPING = 1.0    # gradient clipping \n",
    "# Ornstein-Uhlenbeck noise parameters\n",
    "OU_MEAN = 0.0\n",
    "OU_SIGMA = 0.2\n",
    "OU_THETA = 0.15\n",
    "# Epsilon decay parameters\n",
    "EPSILON = 1.0         \n",
    "EPSILON_DECAY = 1e-5\n",
    "\"\"\"\n",
    "Hyper-parameters for Actor and Critic\n",
    "\"\"\"\n",
    "LR_ACTOR = 1e-3        # learning rate of the actor\n",
    "LR_CRITIC = 1e-3       # learning rate of the critic\n",
    "ACTOR_NN = [N_STATE, 400, 200, N_ACTION]  # hidden layer sizes of the actor network\n",
    "CRITIC_NN = [N_STATE, 400, 200, N_ACTION] # hidden layer sizes of the critic network\n",
    "WEIGHT_INIT = 5e-3     # initialize weights \n",
    "ACTOR_MODEL_PATH = 'DDPG_Tennis_Actor.pth'\n",
    "CRITIC_MODEL_PATH = 'DDPG_Tennis_Critic.pth'\n",
    "# Select GPU for training if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Define the Actor and Critic Network Architecture\n",
    "- The Actor Network learns the policy function and maps states to actions. It consists of three fully connected layers with batch normalization in the second layer. It uses ReLU activation function for the first two layers and uses tanh in the last layer to ensure that the continuous action values range between [-1, 1].\n",
    "- The Critic Network learns the value function and maps (state,action) pairs to Q-values. It also consists of three fully connected layers with batch normalization applied at the first layer. It uses ReLU activation function in all the layers except the last one. The last layer has no activation function so that we get the actual Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=========================================================================\n",
    "                    Actor Model - Learns Policy Function\n",
    "=========================================================================\n",
    "\n",
    "Parameters:\n",
    "-----------\n",
    "NN (list) : Number of nodes in neural network layers       \n",
    "\"\"\"\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, NN=ACTOR_NN):\n",
    "        \"\"\"\n",
    "        Initialize parameters and build the actor model.\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(SEED)\n",
    "        self.H1 = nn.Linear(NN[0], NN[1])\n",
    "        self.H1.weight.data.uniform_(*self.initialize(self.H1))\n",
    "        self.H2 = nn.Linear(NN[1], NN[2])\n",
    "        self.H2.weight.data.uniform_(*self.initialize(self.H2))\n",
    "        self.BN = nn.BatchNorm1d(NN[2])\n",
    "        self.O =  nn.Linear(NN[2], NN[3])\n",
    "        self.O.weight.data.uniform_(-1*WEIGHT_INIT, WEIGHT_INIT)\n",
    "        \n",
    "    def initialize(self, layer):\n",
    "        l = 1. / np.sqrt(layer.weight.data.size()[0])\n",
    "        return(-l, l)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Build an actor(policy) network that maps states to actions\n",
    "        tanh activation ensures that the action values range from [-1, 1]\n",
    "        \"\"\"\n",
    "        x = F.relu(self.H1(state))\n",
    "        x = F.relu(self.BN(self.H2(x)))\n",
    "        return torch.tanh(self.O(x))\n",
    "\"\"\"\n",
    "=========================================================================\n",
    "                Critic Model - Learns Value Function\n",
    "=========================================================================\n",
    "Parameters:\n",
    "-----------\n",
    "NN (list) : Number of nodes in neural network layers\n",
    "\"\"\"  \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, NN=CRITIC_NN):\n",
    "        \"\"\"\n",
    "        Initialize parameters and build the critic model.\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(SEED)\n",
    "        self.H1 = nn.Linear(NN[0], NN[1])\n",
    "        self.H1.weight.data.uniform_(*self.initialize(self.H1))\n",
    "        self.BN = nn.BatchNorm1d(NN[1])\n",
    "        self.H2 = nn.Linear(NN[1]+NN[3], NN[2])\n",
    "        self.H2.weight.data.uniform_(*self.initialize(self.H2))\n",
    "        self.O = nn.Linear(NN[2], 1)\n",
    "        self.O.weight.data.uniform_(-1*WEIGHT_INIT, WEIGHT_INIT)\n",
    "        \n",
    "    def initialize(self, layer):\n",
    "        l = 1. / np.sqrt(layer.weight.data.size()[0])\n",
    "        return(-l, l)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        \"\"\"\n",
    "        Build the critic (value) network to map (state,action) pairs to Q-values\n",
    "        \"\"\"\n",
    "        x = F.relu(self.BN(self.H1(state)))\n",
    "        x = torch.cat((x, action), dim=1)\n",
    "        x = F.relu(self.H2(x))\n",
    "        return self.O(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Define the Replay Memory\n",
    "- For storing experience tuples\n",
    "- For generating mini-batches of experiences for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=========================================================================\n",
    "            Replay Memory to store and sample experiences\n",
    "=========================================================================\n",
    "\"\"\"  \n",
    "class ReplayBuffer:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\"\"\"\n",
    "        self.memory = deque(maxlen=BUFFER_SIZE) \n",
    "        self.experience = namedtuple(\"Experience\", field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(SEED)\n",
    "        \n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store new experience to replay memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory for training\"\"\"\n",
    "        experiences =  random.sample(self.memory, k=BATCH_SIZE)\n",
    "        S = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        A = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        R = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        NS = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        T = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        return (S, A, R, NS, T)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\"Return the current size of internal memory\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Define and Instantiate the DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "=========================================================================\n",
    "            Ornsten-Uhlenbeck process to add exploration noise\n",
    "=========================================================================\n",
    "\"\"\"                  \n",
    "class ExplorationNoise:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize noise parameters\"\"\"\n",
    "        self.mu = OU_MEAN * np.ones(N_ACTION)\n",
    "        self.theta = OU_THETA\n",
    "        self.sigma = OU_SIGMA\n",
    "        self.seed = random.seed(SEED)\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state to mean.\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample\"\"\"\n",
    "        noise = self.theta * (self.mu - self.state) + self.sigma * np.array([random.random() for i in range(len(self.state))])\n",
    "        self.state = self.theta + noise\n",
    "        return self.state\n",
    "\"\"\"\n",
    "=========================================================================\n",
    "    DDPG Agent that interacts with and learns from the environment\n",
    "=========================================================================\n",
    "\"\"\"  \n",
    "class DDPGAgent():   \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize a DDPG Agent object.\"\"\"\n",
    "        self.seed = random.seed(SEED)\n",
    "        self.epsilon = EPSILON\n",
    "        \n",
    "        # Initialize the local and target Actor network\n",
    "        self.actor_local = Actor().to(device)\n",
    "        self.actor_target = Actor().to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "        \n",
    "        # Initialize the local and target Critic network\n",
    "        self.critic_local = Critic().to(device)\n",
    "        self.critic_target = Critic().to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC)\n",
    "        \n",
    "        # Initialize the Replay memory\n",
    "        self.memory = ReplayBuffer()\n",
    "        \n",
    "        # Initialize the Ornsten-Uhlenbeck process to add exploration noise\n",
    "        self.OU_noise = ExplorationNoise()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.OU_noise.reset()\n",
    "        \n",
    "    def step(self, S, A, R, NS, T, timestep):\n",
    "        \"\"\" Store experience in replay memory, and randomly sample experiences from buffer for training \"\"\"\n",
    "        self.memory.store(S, A, R, NS, T)\n",
    "        \"\"\" Learn when enough samples are available in memory \"\"\"\n",
    "        if len(self.memory) > BATCH_SIZE and timestep % LEARN_INTERVAL == 0:\n",
    "            for i in range(LEARN_NUM):\n",
    "                self.learn(self.memory.sample())\n",
    "        \"\"\" epsilon decay \"\"\"\n",
    "        if EPSILON_DECAY > 0:\n",
    "            self.epsilon -= EPSILON_DECAY\n",
    "                \n",
    "    def act(self, S, explore=True):\n",
    "        \"\"\" Returns actions for given state as per currently learnt policy \n",
    "            to which exploration noise is added \"\"\"\n",
    "        S = torch.from_numpy(S).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(S).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if explore:\n",
    "            action += self.epsilon * self.OU_noise.sample()    \n",
    "        return np.clip(action, -1, 1)\n",
    "    \n",
    "    def update(self, local_model, target_model):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target_model = TAU*θ_local_model + (1 - TAU)*θ_target_model\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(TAU*local_param.data + (1.0-TAU)*target_param.data)\n",
    "        \n",
    "    def learn(self, experiences):\n",
    "        \"\"\"Update policy and value parameters using given mini-batch of experience tuples.\n",
    "        Q_targets = r + GAMMA * critic_target(next_state, actor_target(next_state))\n",
    "        \"\"\"\n",
    "        \"\"\" \n",
    "        -----------------------------------------------------------------------------------\n",
    "                                    Unpack mini-batch of experience tuples\n",
    "                    experiences (Tuple[torch.Tensor]): tuple of (S, A, R, NS, T)\n",
    "        -----------------------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        S, A, R, NS, T = experiences\n",
    "        \"\"\" \n",
    "        -----------------------------------------------------------------------------------\n",
    "                                Train Critic by minimizing the loss\n",
    "        -----------------------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        # get predicted actions for next states and Q values from the target critic model\n",
    "        NA = self.actor_target(NS)\n",
    "        Q_targets_next = self.critic_target(NS, NA)\n",
    "        # compute Q-value targets\n",
    "        Q_targets = R + (GAMMA * Q_targets_next * (1 - T))\n",
    "        # compute and minimize the critic loss\n",
    "        Q_expected = self.critic_local(S, A)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        # gradient clipping for critic \n",
    "        torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), GRAD_CLIPPING)\n",
    "        self.critic_optimizer.step()\n",
    "        \"\"\" \n",
    "        -----------------------------------------------------------------------------------\n",
    "                            Train Actor using the sampled policy gradient\n",
    "        -----------------------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        # compute and minimize actor loss\n",
    "        A_pred = self.actor_local(S)\n",
    "        actor_loss = -self.critic_local(S, A_pred).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \"\"\" \n",
    "        -----------------------------------------------------------------------------------\n",
    "                                Update Actor-Critic Target Networks\n",
    "        -----------------------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        self.update(self.critic_local, self.critic_target)\n",
    "        self.update(self.actor_local, self.actor_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Train the DDPG algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50 \tMoving Avg: 0.00580\n",
      "Episode 100 \tMoving Avg: 0.00870\n",
      "Episode 150 \tMoving Avg: 0.00670\n",
      "Episode 200 \tMoving Avg: 0.00390\n",
      "Episode 250 \tMoving Avg: 0.01070\n",
      "Episode 300 \tMoving Avg: 0.02900\n",
      "Episode 350 \tMoving Avg: 0.06690\n",
      "Episode 400 \tMoving Avg: 0.10850\n",
      "Episode 450 \tMoving Avg: 0.13000\n",
      "Episode 500 \tMoving Avg: 0.15450\n",
      "Episode 550 \tMoving Avg: 0.30350\n",
      "Episode 600 \tMoving Avg: 0.49630\n",
      "\n",
      "Environment solved in 614 episodes.\tAverage score: 0.50\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=========================================================================\n",
    "                Train the DDPG Algorithm for multiple agents\n",
    "=========================================================================\n",
    "\"\"\"  \n",
    "def train():\n",
    "    \"\"\" \n",
    "    -----------------------------------------------------------------------------------\n",
    "    Instantiate the DDPG Agent\n",
    "    Initialize data-structures to keep track of average-episode scores and moving average\n",
    "    -----------------------------------------------------------------------------------\n",
    "    \"\"\" \n",
    "    agent = DDPGAgent() # instantiate the DDPG agent\n",
    "    max_score_episode = [] # list of mean scores from each episode\n",
    "    moving_avg_episode = [] # list of moving averages\n",
    "    best_score = -np.inf\n",
    "    scores_last100 = deque(maxlen=100) # mean score from last 100 episodes based on Udacity Project requirements\n",
    "    \"\"\" \n",
    "    -----------------------------------------------------------------------------------\n",
    "    Start learning\n",
    "    -----------------------------------------------------------------------------------\n",
    "    \"\"\" \n",
    "    for i_episode in range(1, N_EPISODES + 1):\n",
    "        \"\"\" Reset environment and agent at the beginning of each episode \"\"\"\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset environment, with training mode ON\n",
    "        scores = np.zeros(num_agents) # initialize score for each agent\n",
    "        states = env_info.vector_observations # get current state \n",
    "        agent.reset() # reset agent\n",
    "        \"\"\" Start episode \"\"\"\n",
    "        for t in range(N_MAX_TIMESTEPS):\n",
    "            \"\"\" Agent interacts with environment \"\"\"\n",
    "            actions = agent.act(states, explore=True) # determine optimal actions\n",
    "            env_info = env.step(actions)[brain_name]  # apply actions to environment and get environment info\n",
    "            next_states = env_info.vector_observations # extract next state information for each agent\n",
    "            rewards = env_info.rewards # extract rewards received for the action taken by each agent\n",
    "            dones = env_info.local_done # Determine terminal state or not\n",
    "            \"\"\" Agent learns from uncorrelated experiences \"\"\"\n",
    "            for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "                agent.step(state, action, reward, next_state, done, t)\n",
    "            states = next_states\n",
    "            scores += rewards\n",
    "            if np.any(dones):\n",
    "                break\n",
    "        \"\"\" \n",
    "        -----------------------------------------------------------------------------------\n",
    "        Record score stats and print information\n",
    "        -----------------------------------------------------------------------------------\n",
    "        \"\"\" \n",
    "        # save max reward collected by the two agents for each episode, which signifies the\n",
    "        # maximum duration for which the ball was in the air. \n",
    "        max_score_episode.append(np.max(scores))  \n",
    "        scores_last100.append(max_score_episode[-1]) # save mean score for last 100 episodes\n",
    "        moving_avg_episode.append(np.mean(scores_last100)) # save moving average\n",
    "        \n",
    "        if i_episode % 50 == 0:\n",
    "            print(\"\\rEpisode {} \\tMoving Avg: {:.5f}\"\\\n",
    "                  .format(i_episode, moving_avg_episode[-1]))\n",
    "        \"\"\" \n",
    "        -----------------------------------------------------------------------------------\n",
    "        Save model weights when the task is solved (moving average score >= 0.5)\n",
    "        -----------------------------------------------------------------------------------\n",
    "        \"\"\" \n",
    "        if moving_avg_episode[-1] >= SCORE_SOLVED:\n",
    "            print(\"\\nEnvironment solved in {:d} episodes.\\tAverage score: {:.2f}\"\\\n",
    "                 .format(i_episode, moving_avg_episode[-1]))\n",
    "            torch.save(agent.actor_local.state_dict(), ACTOR_MODEL_PATH)\n",
    "            torch.save(agent.critic_local.state_dict(), CRITIC_MODEL_PATH)\n",
    "            break        \n",
    "    return max_score_episode, moving_avg_episode\n",
    "\"\"\" \n",
    "-----------------------------------------------------------------------------------\n",
    "Train and save final model weights\n",
    "-----------------------------------------------------------------------------------\n",
    "\"\"\" \n",
    "scores, avgs = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results and Plot of Rewards\n",
    "As indicated above the enviroment was solved in 614 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f011b37c1d0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(np.arange(len(scores)), scores, label='Actual Score ')\n",
    "plt.plot(np.arange(len(scores)), avgs, c='r', label='Moving Average')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode Number')\n",
    "plt.title('Multi-Agent Collaborative Tennis Task using DDPG')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ideas for Future Work\n",
    "- It seems from the result that at first learning performance is slow and after collecting enough episodes, the agent learns to solve the task quickly at the end. I would extend the code to add a separate exploration stage at first, where the agent only explores and collects enough experiences before the agent starts training. Using Prioritized Experience Replay can additionally improve performance.\n",
    "\n",
    "- Because of the multi-agent nature of the task, the training is not always stable. I would implement the strategy in the seminal paper Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
